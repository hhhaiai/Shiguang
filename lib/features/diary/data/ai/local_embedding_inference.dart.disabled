import 'dart:async';
import 'dart:convert';
import 'dart:io';
import 'dart:math' as math;
import 'dart:typed_data';

import 'package:flutter/services.dart';
import 'package:ort/ort.dart';
import 'package:path_provider/path_provider.dart';

/// Local embedding inference using ONNX Runtime.
///
/// Notes:
/// - This model ships with external weights (`model.onnx_data`), so we must
///   materialize files to local disk and load via `commitFromFile`.
/// - Tokenization follows BERT WordPiece from bundled `tokenizer.json`.
class LocalEmbeddingInference {
  static const String _modelAssetPath =
      'assets/models/embedding/bge-small-zh-v1.5-ONNX/onnx/model.onnx';
  static const String _modelDataAssetPath =
      'assets/models/embedding/bge-small-zh-v1.5-ONNX/onnx/model.onnx_data';
  static const String _tokenizerAssetPath =
      'assets/models/embedding/bge-small-zh-v1.5-ONNX/tokenizer.json';

  static const int _maxSequenceLength = 512;
  static const int _embeddingDimensions = 384;

  Session? _session;
  _BertWordPieceTokenizer? _tokenizer;
  Set<String> _inputNames = const {};
  bool _initialized = false;
  Future<void>? _initializationFuture;

  Future<void> ensureInitialized() async {
    if (_initialized && _session != null && _tokenizer != null) return;
    _initializationFuture ??= _initialize();
    await _initializationFuture;
  }

  Future<void> _initialize() async {
    try {
      await Ort.ensureInitialized(throwOnFail: true);
      final modelFilePath = await _ensureModelFilesOnDisk();
      final tokenizer = await _loadTokenizer();

      final session = await Session.builder()
          .withIntraThreads(2)
          .withInterThreads(1)
          .commitFromFile(modelFilePath);

      _session = session;
      _tokenizer = tokenizer;
      _inputNames = session.inputNames.toSet();
      _initialized = true;
    } catch (e) {
      _initialized = false;
      rethrow;
    } finally {
      _initializationFuture = null;
    }
  }

  Future<String> _ensureModelFilesOnDisk() async {
    final supportDir = await getApplicationSupportDirectory();
    final modelDir = Directory(
      '${supportDir.path}/models/embedding/bge-small-zh-v1.5-ONNX/onnx',
    );
    if (!modelDir.existsSync()) {
      await modelDir.create(recursive: true);
    }

    final modelFile = File('${modelDir.path}/model.onnx');
    final modelDataFile = File('${modelDir.path}/model.onnx_data');

    await _copyAssetIfMissing(_modelAssetPath, modelFile);
    await _copyAssetIfMissing(_modelDataAssetPath, modelDataFile);

    return modelFile.path;
  }

  Future<void> _copyAssetIfMissing(String assetPath, File outputFile) async {
    if (outputFile.existsSync() && outputFile.lengthSync() > 0) return;

    final assetData = await rootBundle.load(assetPath);
    final bytes = assetData.buffer.asUint8List();
    await outputFile.writeAsBytes(bytes, flush: true);
  }

  Future<_BertWordPieceTokenizer> _loadTokenizer() async {
    final jsonText = await rootBundle.loadString(_tokenizerAssetPath);
    final payload = jsonDecode(jsonText) as Map<String, dynamic>;
    return _BertWordPieceTokenizer.fromTokenizerJson(payload);
  }

  Future<Float32List> generateEmbedding(String text) async {
    final normalized = text.trim();
    if (normalized.isEmpty) {
      return Float32List(_embeddingDimensions);
    }

    await ensureInitialized();
    final session = _session;
    final tokenizer = _tokenizer;
    if (session == null || tokenizer == null) {
      throw StateError('ONNX embedding runtime is not initialized.');
    }

    final encoded = tokenizer.encode(
      normalized,
      maxSequenceLength: _maxSequenceLength,
    );
    final inputIdsName = _resolveInputName('input_ids');
    final attentionMaskName = _resolveInputName('attention_mask');
    if (inputIdsName == null || attentionMaskName == null) {
      throw StateError(
        'Missing required model inputs: input_ids/attention_mask',
      );
    }

    final inputs = <String, Tensor>{
      inputIdsName: Tensor.fromArrayI64(
        shape: const [1, _maxSequenceLength],
        data: encoded.inputIds,
      ),
      attentionMaskName: Tensor.fromArrayI64(
        shape: const [1, _maxSequenceLength],
        data: encoded.attentionMask,
      ),
    };

    final tokenTypeIdsName = _resolveInputName('token_type_ids');
    if (tokenTypeIdsName != null) {
      inputs[tokenTypeIdsName] = Tensor.fromArrayI64(
        shape: const [1, _maxSequenceLength],
        data: encoded.tokenTypeIds,
      );
    }

    Map<String, Tensor>? outputs;
    try {
      outputs = await session.run(inputValues: inputs);
      return _extractEmbedding(outputs);
    } finally {
      for (final input in inputs.values) {
        input.dispose();
      }
      if (outputs != null) {
        for (final output in outputs.values) {
          output.dispose();
        }
      }
    }
  }

  String? _resolveInputName(String canonicalName) {
    if (_inputNames.contains(canonicalName)) return canonicalName;

    final expected = canonicalName.toLowerCase();
    for (final name in _inputNames) {
      final lower = name.toLowerCase();
      if (lower == expected || lower.contains(expected)) {
        return name;
      }
    }
    return null;
  }

  Float32List _extractEmbedding(Map<String, Tensor> outputs) {
    Tensor? direct = outputs['sentence_embedding'] ?? outputs['embedding'];
    direct ??= outputs.values.firstWhere(
      (t) => t.shape.length == 2 && t.shape.last >= _embeddingDimensions,
      orElse: () => outputs.values.first,
    );

    if (direct.shape.length == 2) {
      return _normalize(_readVector(direct.data));
    }

    if (outputs.containsKey('last_hidden_state')) {
      final hidden = outputs['last_hidden_state']!;
      return _normalize(_readClsFromHidden(hidden));
    }

    final fallbackHidden = outputs.values.firstWhere(
      (t) => t.shape.length == 3 && t.shape.last >= _embeddingDimensions,
      orElse: () => direct!,
    );
    if (fallbackHidden.shape.length == 3) {
      return _normalize(_readClsFromHidden(fallbackHidden));
    }

    throw StateError('Unable to parse embedding output tensor.');
  }

  Float32List _readVector(List<dynamic> data) {
    if (data.length < _embeddingDimensions) {
      throw StateError(
        'Unexpected embedding length: ${data.length}, need $_embeddingDimensions',
      );
    }

    final vector = Float32List(_embeddingDimensions);
    for (var i = 0; i < _embeddingDimensions; i++) {
      vector[i] = (data[i] as num).toDouble();
    }
    return vector;
  }

  Float32List _readClsFromHidden(Tensor hiddenTensor) {
    final shape = hiddenTensor.shape;
    if (shape.length != 3) {
      throw StateError('Expected 3D hidden state output, got shape $shape');
    }

    final seqLen = shape[1];
    final hiddenSize = shape[2];
    if (hiddenSize < _embeddingDimensions) {
      throw StateError(
        'Hidden size $hiddenSize is smaller than expected $_embeddingDimensions',
      );
    }

    final data = hiddenTensor.data;
    final clsOffset = 0 * seqLen * hiddenSize;
    final vector = Float32List(_embeddingDimensions);
    for (var i = 0; i < _embeddingDimensions; i++) {
      vector[i] = (data[clsOffset + i] as num).toDouble();
    }
    return vector;
  }

  Float32List _normalize(Float32List embedding) {
    var sumSquares = 0.0;
    for (final value in embedding) {
      sumSquares += value * value;
    }
    final norm = sumSquares > 0 ? math.sqrt(sumSquares) : 1.0;

    final normalized = Float32List(embedding.length);
    for (var i = 0; i < embedding.length; i++) {
      normalized[i] = embedding[i] / norm;
    }
    return normalized;
  }

  bool get isInitialized => _initialized;

  void dispose() {
    _session?.dispose();
    _session = null;
    _tokenizer = null;
    _inputNames = const {};
    _initialized = false;
  }
}

class _TokenizedText {
  final List<int> inputIds;
  final List<int> attentionMask;
  final List<int> tokenTypeIds;

  const _TokenizedText({
    required this.inputIds,
    required this.attentionMask,
    required this.tokenTypeIds,
  });
}

class _BertWordPieceTokenizer {
  static const int _maxInputCharsPerWord = 100;

  final Map<String, int> _vocab;
  final int _padTokenId;
  final int _unkTokenId;
  final int _clsTokenId;
  final int _sepTokenId;
  final bool _cleanText;
  final bool _handleChineseChars;
  final bool _lowercase;

  _BertWordPieceTokenizer({
    required Map<String, int> vocab,
    required int padTokenId,
    required int unkTokenId,
    required int clsTokenId,
    required int sepTokenId,
    required bool cleanText,
    required bool handleChineseChars,
    required bool lowercase,
  }) : _vocab = vocab,
       _padTokenId = padTokenId,
       _unkTokenId = unkTokenId,
       _clsTokenId = clsTokenId,
       _sepTokenId = sepTokenId,
       _cleanText = cleanText,
       _handleChineseChars = handleChineseChars,
       _lowercase = lowercase;

  factory _BertWordPieceTokenizer.fromTokenizerJson(
    Map<String, dynamic> tokenizerJson,
  ) {
    final model = tokenizerJson['model'] as Map<String, dynamic>? ?? const {};
    final rawVocab = model['vocab'] as Map<String, dynamic>? ?? const {};
    final vocab = <String, int>{};
    for (final entry in rawVocab.entries) {
      vocab[entry.key] = (entry.value as num).toInt();
    }

    final normalizer =
        tokenizerJson['normalizer'] as Map<String, dynamic>? ?? const {};
    final cleanText = normalizer['clean_text'] as bool? ?? true;
    final handleChineseChars =
        normalizer['handle_chinese_chars'] as bool? ?? true;
    final lowercase = normalizer['lowercase'] as bool? ?? false;
    final unkToken = model['unk_token'] as String? ?? '[UNK]';

    return _BertWordPieceTokenizer(
      vocab: vocab,
      padTokenId: vocab['[PAD]'] ?? 0,
      unkTokenId: vocab[unkToken] ?? 100,
      clsTokenId: vocab['[CLS]'] ?? 101,
      sepTokenId: vocab['[SEP]'] ?? 102,
      cleanText: cleanText,
      handleChineseChars: handleChineseChars,
      lowercase: lowercase,
    );
  }

  _TokenizedText encode(String text, {required int maxSequenceLength}) {
    final normalizedText = _normalize(text);
    final basicTokens = _basicTokenize(normalizedText);

    final wordPieceIds = <int>[];
    for (final token in basicTokens) {
      wordPieceIds.addAll(_wordPieceTokenize(token));
    }

    final payloadCapacity = maxSequenceLength - 2; // [CLS] + [SEP]
    final contentIds = wordPieceIds.length > payloadCapacity
        ? wordPieceIds.sublist(0, payloadCapacity)
        : wordPieceIds;
    final ids = <int>[_clsTokenId, ...contentIds, _sepTokenId];

    final inputIds = List<int>.filled(maxSequenceLength, _padTokenId);
    final attentionMask = List<int>.filled(maxSequenceLength, 0);
    final tokenTypeIds = List<int>.filled(maxSequenceLength, 0);

    for (var i = 0; i < ids.length; i++) {
      inputIds[i] = ids[i];
      attentionMask[i] = 1;
    }

    return _TokenizedText(
      inputIds: inputIds,
      attentionMask: attentionMask,
      tokenTypeIds: tokenTypeIds,
    );
  }

  String _normalize(String text) {
    final buffer = StringBuffer();
    for (final rune in text.runes) {
      if (_cleanText && _isControl(rune)) continue;

      if (_isWhitespace(rune)) {
        buffer.write(' ');
        continue;
      }

      if (_handleChineseChars && _isChineseChar(rune)) {
        buffer.write(' ');
        buffer.writeCharCode(rune);
        buffer.write(' ');
      } else {
        buffer.writeCharCode(rune);
      }
    }

    var normalized = buffer.toString();
    if (_lowercase) {
      normalized = normalized.toLowerCase();
    }
    return normalized.trim();
  }

  List<String> _basicTokenize(String text) {
    if (text.isEmpty) return const [];

    final tokens = <String>[];
    for (final part in text.split(RegExp(r'\s+'))) {
      if (part.isEmpty) continue;
      tokens.addAll(_splitOnPunctuation(part));
    }
    return tokens;
  }

  List<String> _splitOnPunctuation(String token) {
    final output = <String>[];
    final current = <int>[];

    for (final rune in token.runes) {
      if (_isPunctuation(rune)) {
        if (current.isNotEmpty) {
          output.add(String.fromCharCodes(current));
          current.clear();
        }
        output.add(String.fromCharCode(rune));
      } else {
        current.add(rune);
      }
    }

    if (current.isNotEmpty) {
      output.add(String.fromCharCodes(current));
    }
    return output;
  }

  List<int> _wordPieceTokenize(String token) {
    final runes = token.runes.toList(growable: false);
    if (runes.length > _maxInputCharsPerWord) {
      return <int>[_unkTokenId];
    }

    final result = <int>[];
    var start = 0;
    while (start < runes.length) {
      var end = runes.length;
      int? matchedId;

      while (start < end) {
        var piece = String.fromCharCodes(runes.sublist(start, end));
        if (start > 0) {
          piece = '##$piece';
        }

        final tokenId = _vocab[piece];
        if (tokenId != null) {
          matchedId = tokenId;
          break;
        }
        end -= 1;
      }

      if (matchedId == null) {
        return <int>[_unkTokenId];
      }

      result.add(matchedId);
      start = end;
    }

    return result;
  }

  bool _isWhitespace(int rune) =>
      rune == 0x20 ||
      rune == 0x09 ||
      rune == 0x0A ||
      rune == 0x0D ||
      rune == 0x00A0;

  bool _isControl(int rune) {
    if (rune == 0x09 || rune == 0x0A || rune == 0x0D) {
      return false;
    }
    return (rune < 0x20) || (rune >= 0x7F && rune <= 0x9F);
  }

  bool _isPunctuation(int rune) {
    final isAsciiPunctuation =
        (rune >= 33 && rune <= 47) ||
        (rune >= 58 && rune <= 64) ||
        (rune >= 91 && rune <= 96) ||
        (rune >= 123 && rune <= 126);
    if (isAsciiPunctuation) return true;

    return (rune >= 0x2000 && rune <= 0x206F) ||
        (rune >= 0x2E00 && rune <= 0x2E7F);
  }

  bool _isChineseChar(int rune) {
    return (rune >= 0x4E00 && rune <= 0x9FFF) ||
        (rune >= 0x3400 && rune <= 0x4DBF) ||
        (rune >= 0x20000 && rune <= 0x2A6DF) ||
        (rune >= 0x2A700 && rune <= 0x2B73F) ||
        (rune >= 0x2B740 && rune <= 0x2B81F) ||
        (rune >= 0x2B820 && rune <= 0x2CEAF) ||
        (rune >= 0xF900 && rune <= 0xFAFF) ||
        (rune >= 0x2F800 && rune <= 0x2FA1F);
  }
}
